"""
LLM Client - Model Ä°stemcisi
==============================
OpenAI API ile iletiÅŸim kuran ana istemci sÄ±nÄ±fÄ±.

Bu dosya ne yapar?
-----------------
1. OpenAI API'ye baÄŸlanÄ±r
2. Mesaj gÃ¶nderir ve cevap alÄ±r
3. Tool Ã§aÄŸrÄ±larÄ±nÄ± destekler
4. Token kullanÄ±mÄ±nÄ± takip eder

KullanÄ±m:
    from shared.llm.client import LLMClient
    
    client = LLMClient()
    
    # Basit sohbet
    response = await client.chat("Merhaba, nasÄ±lsÄ±n?")
    print(response.content)
    
    # Tool'larla birlikte
    response = await client.chat(
        message="Ä°stanbul'da hava nasÄ±l?",
        tools=[weather_tool_schema]
    )
"""

import os
import json
from typing import Optional
from dataclasses import dataclass, field
from dotenv import load_dotenv

# .env dosyasÄ±ndan API key'i yÃ¼kle
load_dotenv()

try:
    from openai import AsyncOpenAI
except ImportError:
    AsyncOpenAI = None


# ============================================================
# Veri SÄ±nÄ±flarÄ± (Data Classes)
# ============================================================
# Bu sÄ±nÄ±flar, LLM'den dÃ¶nen cevaplarÄ± dÃ¼zenli tutmamÄ±zÄ± saÄŸlar

@dataclass
class ToolCall:
    """
    LLM'in Ã§aÄŸÄ±rmak istediÄŸi bir tool'u temsil eder.
    
    Ã–rnek:
        tool_call = ToolCall(
            id="call_123",
            name="get_weather",
            arguments={"city": "Istanbul"}
        )
    """
    id: str                    # Tool Ã§aÄŸrÄ±sÄ±nÄ±n benzersiz kimliÄŸi
    name: str                  # Tool'un adÄ± (Ã¶rn: "get_weather")
    arguments: dict            # Tool'a gÃ¶nderilecek parametreler


@dataclass
class TokenUsage:
    """
    Bir LLM Ã§aÄŸrÄ±sÄ±nda kullanÄ±lan token miktarÄ±.
    
    Token nedir?
    - LLM'lerin metni iÅŸlediÄŸi en kÃ¼Ã§Ã¼k birim
    - YaklaÅŸÄ±k 1 token â‰ˆ 4 karakter (Ä°ngilizce)
    - Her Ã§aÄŸrÄ± para! Bu yÃ¼zden takip etmek Ã¶nemli
    """
    input_tokens: int = 0      # GÃ¶nderdiÄŸimiz metin (prompt)
    output_tokens: int = 0     # LLM'in Ã¼rettiÄŸi metin (cevap)
    
    @property
    def total_tokens(self) -> int:
        """Toplam token sayÄ±sÄ±"""
        return self.input_tokens + self.output_tokens
    
    def estimate_cost(self, model: str = "gpt-4o-mini") -> float:
        """
        Tahmini maliyet hesapla (USD).
        
        Not: Fiyatlar deÄŸiÅŸebilir, gÃ¼ncel fiyatlar iÃ§in OpenAI'Ä± kontrol edin.
        """
        # YaklaÅŸÄ±k fiyatlar (USD per 1M token)
        pricing = {
            "gpt-4o-mini": {"input": 0.15, "output": 0.60},
            "gpt-4o": {"input": 2.50, "output": 10.00},
            "gpt-4-turbo": {"input": 10.00, "output": 30.00},
        }
        
        prices = pricing.get(model, pricing["gpt-4o-mini"])
        
        input_cost = (self.input_tokens / 1_000_000) * prices["input"]
        output_cost = (self.output_tokens / 1_000_000) * prices["output"]
        
        return input_cost + output_cost


@dataclass
class LLMResponse:
    """
    LLM'den dÃ¶nen cevabÄ± temsil eder.
    
    Ä°ki tÃ¼r cevap olabilir:
    1. content: Normal metin cevabÄ± ("Hava gÃ¼neÅŸli")
    2. tool_calls: Tool Ã§aÄŸrÄ±sÄ± isteÄŸi (get_weather Ã§aÄŸÄ±r)
    """
    content: Optional[str] = None          # Metin cevabÄ±
    tool_calls: list[ToolCall] = field(default_factory=list)  # Tool Ã§aÄŸrÄ±larÄ±
    usage: TokenUsage = field(default_factory=TokenUsage)     # Token kullanÄ±mÄ±
    model: str = ""                        # KullanÄ±lan model
    
    @property
    def has_tool_calls(self) -> bool:
        """Tool Ã§aÄŸrÄ±sÄ± var mÄ±?"""
        return len(self.tool_calls) > 0


# ============================================================
# Ana LLM Client SÄ±nÄ±fÄ±
# ============================================================

class LLMClient:
    """
    OpenAI API ile iletiÅŸim kuran ana istemci.
    
    Bu sÄ±nÄ±f ne yapar?
    1. API baÄŸlantÄ±sÄ±nÄ± yÃ¶netir
    2. Mesaj geÃ§miÅŸini tutar (isteÄŸe baÄŸlÄ±)
    3. Token kullanÄ±mÄ±nÄ± takip eder
    4. Tool Ã§aÄŸrÄ±larÄ±nÄ± destekler
    
    KullanÄ±m:
        client = LLMClient(model="gpt-4o-mini")
        
        # Basit kullanÄ±m
        response = await client.chat("Merhaba!")
        print(response.content)
        
        # Mesaj geÃ§miÅŸi ile
        messages = [
            {"role": "system", "content": "Sen yardÄ±mcÄ± bir asistansÄ±n."},
            {"role": "user", "content": "Python nedir?"},
        ]
        response = await client.chat_with_messages(messages)
    """
    
    def __init__(
        self,
        model: str = None,
        api_key: str = None,
        temperature: float = 0.7,
    ):
        """
        LLMClient'Ä± baÅŸlat.
        
        Parametreler:
            model: KullanÄ±lacak model (varsayÄ±lan: .env'den veya gpt-4o-mini)
            api_key: OpenAI API key (varsayÄ±lan: .env'den)
            temperature: YaratÄ±cÄ±lÄ±k seviyesi (0=deterministik, 1=yaratÄ±cÄ±)
        """
        self.model = model or os.getenv("DEFAULT_MODEL", "gpt-4o-mini")
        self.temperature = temperature
        
        # Toplam token kullanÄ±mÄ±nÄ± takip et
        self.total_usage = TokenUsage()
        
        # API istemcisini oluÅŸtur
        resolved_key = api_key or os.getenv("OPENAI_API_KEY")
        
        if AsyncOpenAI is None:
            print("âš ï¸  openai paketi yÃ¼klÃ¼ deÄŸil. 'pip install openai' Ã§alÄ±ÅŸtÄ±rÄ±n.")
            self._client = None
        elif not resolved_key or resolved_key == "sk-your-api-key-here":
            print("âš ï¸  OPENAI_API_KEY ayarlanmamÄ±ÅŸ. .env dosyanÄ±zÄ± kontrol edin.")
            self._client = None
        else:
            self._client = AsyncOpenAI(api_key=resolved_key)
    
    async def chat(
        self,
        message: str,
        system_prompt: str = None,
        tools: list[dict] = None,
    ) -> LLMResponse:
        """
        Basit bir mesaj gÃ¶nder ve cevap al.
        
        Parametreler:
            message: KullanÄ±cÄ± mesajÄ±
            system_prompt: Sistem talimatÄ± (isteÄŸe baÄŸlÄ±)
            tools: KullanÄ±labilir tool ÅŸemalarÄ± (isteÄŸe baÄŸlÄ±)
        
        DÃ¶ndÃ¼rÃ¼r:
            LLMResponse: Model cevabÄ±
        
        Ã–rnek:
            response = await client.chat("Python nedir?")
            print(response.content)
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": message})
        
        return await self.chat_with_messages(messages, tools=tools)
    
    async def chat_with_messages(
        self,
        messages: list[dict],
        tools: list[dict] = None,
    ) -> LLMResponse:
        """
        Mesaj listesi ile LLM'e istek gÃ¶nder.
        
        Bu method daha geliÅŸmiÅŸ kullanÄ±m iÃ§indir.
        Mesaj geÃ§miÅŸini kontrol etmek istediÄŸinizde kullanÄ±n.
        
        Parametreler:
            messages: Mesaj listesi [{"role": "...", "content": "..."}]
            tools: KullanÄ±labilir tool ÅŸemalarÄ±
        
        DÃ¶ndÃ¼rÃ¼r:
            LLMResponse: Model cevabÄ±
        """
        # API istemcisi yoksa demo mod
        if self._client is None:
            return self._demo_response(messages, tools)
        
        # API Ã§aÄŸrÄ±sÄ± iÃ§in parametreler
        params = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
        }
        
        # Tool'lar varsa ekle
        if tools:
            params["tools"] = tools
            params["tool_choice"] = "auto"  # Model tool kullanÄ±p kullanmamaya karar verir
        
        # API Ã§aÄŸrÄ±sÄ± yap
        response = await self._client.chat.completions.create(**params)
        
        # CevabÄ± parse et
        return self._parse_response(response)
    
    def _parse_response(self, response) -> LLMResponse:
        """API cevabÄ±nÄ± LLMResponse'a dÃ¶nÃ¼ÅŸtÃ¼r."""
        message = response.choices[0].message
        
        # Token kullanÄ±mÄ±
        usage = TokenUsage(
            input_tokens=response.usage.prompt_tokens,
            output_tokens=response.usage.completion_tokens,
        )
        
        # Toplam kullanÄ±mÄ± gÃ¼ncelle
        self.total_usage.input_tokens += usage.input_tokens
        self.total_usage.output_tokens += usage.output_tokens
        
        # Tool Ã§aÄŸrÄ±larÄ±
        tool_calls = []
        if message.tool_calls:
            for tc in message.tool_calls:
                tool_calls.append(ToolCall(
                    id=tc.id,
                    name=tc.function.name,
                    arguments=json.loads(tc.function.arguments),
                ))
        
        return LLMResponse(
            content=message.content,
            tool_calls=tool_calls,
            usage=usage,
            model=response.model,
        )
    
    def _demo_response(self, messages: list[dict], tools: list[dict] = None) -> LLMResponse:
        """
        API key yoksa demo cevap dÃ¶ndÃ¼r.
        Bu sayede API key olmadan da kodu test edebilirsiniz.
        """
        last_message = messages[-1]["content"] if messages else ""
        
        # Tool varsa demo tool Ã§aÄŸrÄ±sÄ± yap
        if tools and len(tools) > 0:
            first_tool = tools[0]
            tool_name = first_tool["function"]["name"]
            
            return LLMResponse(
                content=None,
                tool_calls=[ToolCall(
                    id="demo_call_001",
                    name=tool_name,
                    arguments={"input": last_message},
                )],
                usage=TokenUsage(input_tokens=50, output_tokens=20),
                model="demo-mode",
            )
        
        # Tool yoksa metin cevabÄ± ver
        return LLMResponse(
            content=f"[DEMO MOD] MesajÄ±nÄ±z alÄ±ndÄ±: '{last_message[:50]}...' "
                    f"(GerÃ§ek cevap iÃ§in OPENAI_API_KEY ayarlayÄ±n)",
            usage=TokenUsage(input_tokens=50, output_tokens=30),
            model="demo-mode",
        )
    
    def get_usage_report(self) -> str:
        """
        Toplam token kullanÄ±m raporu dÃ¶ndÃ¼r.
        
        Ã–rnek Ã§Ä±ktÄ±:
            ğŸ“Š Token KullanÄ±m Raporu
            Model: gpt-4o-mini
            Input:  1500 tokens
            Output:  500 tokens
            Toplam: 2000 tokens
            Tahmini Maliyet: $0.000525
        """
        cost = self.total_usage.estimate_cost(self.model)
        return (
            f"ğŸ“Š Token KullanÄ±m Raporu\n"
            f"   Model:  {self.model}\n"
            f"   Input:  {self.total_usage.input_tokens:,} tokens\n"
            f"   Output: {self.total_usage.output_tokens:,} tokens\n"
            f"   Toplam: {self.total_usage.total_tokens:,} tokens\n"
            f"   Tahmini Maliyet: ${cost:.6f}"
        )
